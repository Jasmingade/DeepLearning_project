{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEyE/DWFccaXvmzPQZHECM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jasmingade/DeepLearning_project/blob/main/02_dat_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Deep Learning Project 29**\n",
        "# Prediction of protein isoforms using semi-supervised learning\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "4mXSdbIM4Rth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ApVVVq794u3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning the data\n"
      ],
      "metadata": {
        "id": "0fKNRbvm42qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define chunk size\n",
        "chunk_size = 10000  # Adjust this to a value that your Colab environment can handle\n",
        "\n",
        "# Create an empty DataFrame to hold the cleaned data\n",
        "cleaned_data = pd.DataFrame()\n",
        "\n",
        "# Use an iterator to load the dataset in chunks\n",
        "for chunk in pd.read_csv('path_to_large_file.csv.gz', chunksize=chunk_size, compression='gzip'):\n",
        "\n",
        "    # Perform cleaning operations here.\n",
        "    # For example, drop rows with missing values, filter out unwanted data, transform columns, etc.\n",
        "    chunk.dropna(inplace=True)  # Just an example operation\n",
        "\n",
        "    # Once the chunk is cleaned, you can either process it or append it to the cleaned_data DataFrame\n",
        "    cleaned_data = pd.concat([cleaned_data, chunk], ignore_index=True)\n",
        "\n",
        "    # Depending on your needs, you can also save the cleaned chunks to new files\n",
        "    # or aggregate data from chunks before saving.\n",
        "\n",
        "# Once all chunks have been processed, cleaned_data will hold all the cleaned data\n",
        "\n",
        "# You can save the cleaned data to a new file if needed\n",
        "cleaned_data.to_csv('cleaned_data_file.csv', index=False)\n"
      ],
      "metadata": {
        "id": "rlBgKvyS46WN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}